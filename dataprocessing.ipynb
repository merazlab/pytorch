{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataprocessing.ipynb",
      "provenance": [],
      "mount_file_id": "1Od1DAwtRMWw8C9jSwHpoW330BKI6KAch",
      "authorship_tag": "ABX9TyNwBeDIxQFuTtq+5O2H5gUG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merazlab/pytorch/blob/master/dataprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYu59M0SYd2H",
        "colab_type": "code",
        "outputId": "0d221b1f-d9ed-49d7-e360-75975395b6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "cd /content/drive/My Drive/pytorch"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXfrM2hDZrwW",
        "colab_type": "code",
        "outputId": "522e81f8-4386-4f56-f03e-2b1fb05ea10e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ls\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataprocessing.ipynb  mod_01_Intro.ipnb  Untitled0.ipynb\n",
            "diabetes.csv          mod_02_01.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X244LwzTZts-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn  as nn\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqz83dzSaC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"diabetes.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn7ooQNTcHSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = data.iloc[:, 0:-1]\n",
        "y_string = list(data.iloc[:,-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyXiOtQDefuC",
        "colab_type": "code",
        "outputId": "bfb76b50-ae51-498f-fa61-a55b4f015bbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(type(x))\n",
        "print(type(y_string))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUHf0-GlelXo",
        "colab_type": "code",
        "outputId": "a4977bbe-d79d-4ff5-f543-d75ac915cd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "print(x[:3])"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Number of times pregnant  Plasma glucose concentration  ...  Body mass index  Age\n",
            "0                         6                           148  ...             33.6   50\n",
            "1                         1                            85  ...             26.6   31\n",
            "2                         8                           183  ...             23.3   32\n",
            "\n",
            "[3 rows x 7 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoUZ113EfeZm",
        "colab_type": "code",
        "outputId": "cf50f509-cc1b-4ea4-e05f-e5565536f196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(y_string)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4m_QIrTe4oC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_int = []\n",
        "         \n",
        "for val in y_string: \n",
        "  if val == \"positive\":\n",
        "    y_int.append(1)\n",
        "  else:\n",
        "    y_int.append(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKciezCEoAf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = np.array(y_int, dtype=\"float64\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjSwtuDJoZCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc  = StandardScaler()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtwDkBc8o29P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = sc.fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZv4RcGpEDK",
        "colab_type": "code",
        "outputId": "f19baa4c-8c72-4325-f192-9be75d5f9a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(x))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iDPpccBpF_d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x= torch.tensor(x)\n",
        "y = torch.tensor(y).view(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMQ5oVaSpdFW",
        "colab_type": "code",
        "outputId": "2b442d65-0a50-41cb-9dd1-ec9e0898d797",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(x.size())\n",
        "print(y.size())"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([768, 7])\n",
            "torch.Size([768, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDZM3lATpiXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(Dataset):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "  def __getitem__(self, index):\n",
        "    return self.x[index], self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    # return len(self.y)\n",
        "    return len(self.x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmoJT5sDxXep",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = Dataset(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rklsDKZWxjmV",
        "colab_type": "code",
        "outputId": "3e55baf5-d26a-4b8f-cb1d-1312629be414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-yk14x98l7q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "72c6da9c-9a3b-400a-dd37-5b577aacb125"
      },
      "source": [
        "first_data = dataset[0]\n",
        "feature, label = first_data\n",
        "print(feature)\n",
        "print(label)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.6399,  0.8483,  0.1496,  0.9073, -0.6929,  0.2040,  1.4260],\n",
            "       dtype=torch.float64)\n",
            "tensor([1.], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LihvybUb8kgY",
        "colab_type": "text"
      },
      "source": [
        "Now Batch size concept"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5k7AWN5xsOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loder = torch.utils.data.DataLoader(dataset=dataset, batch_size=4, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdaB2QY_7trX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "8920905c-c60f-4580-c593-3629abca5cbb"
      },
      "source": [
        "data_iter = iter(train_loder)\n",
        "data_batch = next(data_iter)\n",
        "\n",
        "feature, label = data_batch\n",
        "print(feature)\n",
        "print(label)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2510,  2.1941, -0.0572, -0.3473,  0.4359, -0.1387,  0.0646],\n",
            "        [ 0.3430,  1.1613,  0.7700,  1.2836,  1.1305,  0.9401, -0.3608],\n",
            "        [-1.1419, -0.0593, -0.2639, -0.1591,  0.1059,  0.3690, -0.8714],\n",
            "        [-0.2510, -0.2471, -1.2979, -0.4727, -0.6929, -1.2175, -0.9565]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j8saWse70Gu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "0b5f5474-4e1b-40cc-a9aa-17abbbb2600c"
      },
      "source": [
        "for i, (feature, label) in enumerate(train_loder):\n",
        "  print(i,  feature, label)\n",
        "  if i == 2: break"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([[-0.2510,  1.1613,  0.0462,  0.5936,  2.1551,  0.4452,  0.1497],\n",
            "        [-0.8449, -0.5913,  0.2530, -1.2882, -0.6929,  0.9528,  0.7453],\n",
            "        [-1.1419,  1.8498,  1.0802,  0.3427,  0.0886,  0.5721,  0.1497],\n",
            "        [ 0.0460, -0.1219, -0.3673, -0.5355, -0.6929, -0.2910, -0.2758]],\n",
            "       dtype=torch.float64) tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], dtype=torch.float64)\n",
            "1 tensor([[-0.2510,  1.1613,  0.3564,  0.9700,  1.4344, -0.0498, -0.4459],\n",
            "        [ 0.6399, -1.2799,  0.5632,  0.9700, -0.6929,  0.9909, -0.4459],\n",
            "        [ 1.2339,  2.1002,  0.4598, -1.2882, -0.6929,  2.0190,  0.8304],\n",
            "        [ 0.3430,  0.8170,  0.3047, -1.2882, -0.6929, -0.2656, -0.4459]],\n",
            "       dtype=torch.float64) tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.]], dtype=torch.float64)\n",
            "2 tensor([[-0.8449,  0.0346, -0.2639,  0.7191,  0.6616,  0.3944, -0.2758],\n",
            "        [-0.2510,  0.2224,  0.1496,  0.2800,  0.9569,  0.0517, -0.5310],\n",
            "        [ 0.0460,  1.0987,  0.3047, -1.2882, -0.6929,  2.0697, -0.1056],\n",
            "        [-1.1419,  0.5979, -0.2122,  0.3427,  0.4359,  1.3463, -0.7863]],\n",
            "       dtype=torch.float64) tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH5s4xFnCIek",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l__m7HNvDo4b",
        "colab_type": "text"
      },
      "source": [
        "Linear layer == MLP = Fcn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0UwTluhAtMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, input_features, output_features):\n",
        "    super(Model, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_features, 5)\n",
        "    self.fc2 = nn.Linear(5, 4)\n",
        "    self.fc3 = nn.Linear(4, 3)\n",
        "    self.fc4 = nn.Linear(3, output_features)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.fc1(x)\n",
        "    out= self.tanh(out)\n",
        "    out = self.fc2(out)\n",
        "    out= self.tanh(out)\n",
        "    out = self.fc3(out)\n",
        "    out= self.tanh(out)\n",
        "    out = self.fc4(out)\n",
        "    out= self.sigmoid(out)\n",
        "    return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i_lsnxnGEfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "ca19a613-1b00-4530-d580-783b242f08b1"
      },
      "source": [
        "net = Model(7, 1)\n",
        "criterion = torch.nn.BCELoss(size_average=True) # due to batch\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr = 0.1, momentum=0.9)\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efPdxOX3HZAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "156d1207-fbab-49d6-bece-d084ad515189"
      },
      "source": [
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "  for inputs, labels in train_loder:\n",
        "    inputs = inputs.float()\n",
        "    labels = labels.float()\n",
        "    # foreard\n",
        "    outputs = net(inputs)\n",
        "    #Loss calculation\n",
        "    loss = criterion(outputs, labels)\n",
        "    # print(inputs.dtype)\n",
        "    # print(labels.dtype)\n",
        "    # print(outputs.dtype)\n",
        "    # print(loss.dtype)\n",
        "    # break\n",
        "    #clear gradiant buffer\n",
        "    optimizer.zero_grad()\n",
        "    #calculate gradiant W <<--- W - lr*gradiant\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  \n",
        "  #Accuracy calculation\n",
        "  outputs = (outputs > 0.5)\n",
        "  # print(outputs.dtype)\n",
        "  outputs = outputs.float()\n",
        "  # print(outputs.dtype)\n",
        "  accuracy = (outputs == labels).float().mean()\n",
        "  print(accuracy.dtype)\n",
        "  print(\"Epoch{}/{}, loss: {:.3f}, Accuracy: {:.3f}\". format(epoch+1, epochs, loss, accuracy))\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "Epoch1/200, loss: 0.671, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch2/200, loss: 0.161, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch3/200, loss: 0.824, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch4/200, loss: 0.399, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch5/200, loss: 0.728, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch6/200, loss: 0.577, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch7/200, loss: 0.273, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch8/200, loss: 0.709, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch9/200, loss: 0.682, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch10/200, loss: 0.661, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch11/200, loss: 0.520, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch12/200, loss: 0.219, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch13/200, loss: 0.517, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch14/200, loss: 0.307, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch15/200, loss: 1.162, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch16/200, loss: 0.863, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch17/200, loss: 0.774, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch18/200, loss: 0.266, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch19/200, loss: 0.684, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch20/200, loss: 0.715, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch21/200, loss: 0.528, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch22/200, loss: 0.778, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch23/200, loss: 1.366, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch24/200, loss: 0.459, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch25/200, loss: 0.674, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch26/200, loss: 0.731, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch27/200, loss: 0.202, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch28/200, loss: 0.431, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch29/200, loss: 0.079, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch30/200, loss: 0.599, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch31/200, loss: 0.194, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch32/200, loss: 1.005, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch33/200, loss: 0.163, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch34/200, loss: 0.663, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch35/200, loss: 0.211, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch36/200, loss: 0.672, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch37/200, loss: 0.131, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch38/200, loss: 0.349, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch39/200, loss: 0.498, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch40/200, loss: 0.129, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch41/200, loss: 0.430, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch42/200, loss: 0.585, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch43/200, loss: 0.367, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch44/200, loss: 0.790, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch45/200, loss: 0.342, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch46/200, loss: 0.720, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch47/200, loss: 0.359, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch48/200, loss: 0.807, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch49/200, loss: 0.097, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch50/200, loss: 0.600, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch51/200, loss: 0.484, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch52/200, loss: 0.257, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch53/200, loss: 1.166, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch54/200, loss: 0.558, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch55/200, loss: 0.319, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch56/200, loss: 0.611, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch57/200, loss: 0.701, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch58/200, loss: 0.364, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch59/200, loss: 1.372, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch60/200, loss: 0.171, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch61/200, loss: 0.284, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch62/200, loss: 0.676, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch63/200, loss: 0.402, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch64/200, loss: 0.480, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch65/200, loss: 0.471, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch66/200, loss: 0.494, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch67/200, loss: 0.869, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch68/200, loss: 0.183, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch69/200, loss: 0.600, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch70/200, loss: 0.267, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch71/200, loss: 0.781, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch72/200, loss: 1.231, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch73/200, loss: 0.121, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch74/200, loss: 0.464, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch75/200, loss: 0.100, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch76/200, loss: 0.474, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch77/200, loss: 0.331, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch78/200, loss: 0.582, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch79/200, loss: 0.420, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch80/200, loss: 0.854, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch81/200, loss: 0.521, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch82/200, loss: 1.236, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch83/200, loss: 0.596, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch84/200, loss: 0.697, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch85/200, loss: 0.110, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch86/200, loss: 0.293, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch87/200, loss: 1.423, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch88/200, loss: 0.846, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch89/200, loss: 0.472, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch90/200, loss: 0.165, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch91/200, loss: 0.361, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch92/200, loss: 0.429, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch93/200, loss: 0.401, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch94/200, loss: 1.038, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch95/200, loss: 0.503, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch96/200, loss: 0.445, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch97/200, loss: 0.705, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch98/200, loss: 0.079, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch99/200, loss: 0.239, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch100/200, loss: 0.848, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch101/200, loss: 0.571, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch102/200, loss: 0.511, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch103/200, loss: 0.933, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch104/200, loss: 0.481, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch105/200, loss: 0.193, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch106/200, loss: 0.440, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch107/200, loss: 0.760, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch108/200, loss: 2.020, Accuracy: 0.000\n",
            "torch.float32\n",
            "Epoch109/200, loss: 0.482, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch110/200, loss: 0.930, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch111/200, loss: 0.486, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch112/200, loss: 0.142, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch113/200, loss: 0.144, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch114/200, loss: 0.941, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch115/200, loss: 0.819, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch116/200, loss: 0.671, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch117/200, loss: 0.492, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch118/200, loss: 0.278, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch119/200, loss: 0.215, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch120/200, loss: 0.626, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch121/200, loss: 0.214, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch122/200, loss: 0.238, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch123/200, loss: 0.255, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch124/200, loss: 0.044, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch125/200, loss: 1.048, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch126/200, loss: 0.779, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch127/200, loss: 0.529, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch128/200, loss: 0.172, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch129/200, loss: 0.385, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch130/200, loss: 0.894, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch131/200, loss: 0.688, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch132/200, loss: 0.766, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch133/200, loss: 0.751, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch134/200, loss: 0.602, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch135/200, loss: 0.152, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch136/200, loss: 0.391, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch137/200, loss: 0.615, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch138/200, loss: 0.612, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch139/200, loss: 0.708, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch140/200, loss: 0.210, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch141/200, loss: 0.275, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch142/200, loss: 1.411, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch143/200, loss: 0.353, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch144/200, loss: 0.084, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch145/200, loss: 0.169, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch146/200, loss: 0.799, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch147/200, loss: 0.714, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch148/200, loss: 0.440, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch149/200, loss: 0.249, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch150/200, loss: 0.330, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch151/200, loss: 0.492, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch152/200, loss: 0.111, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch153/200, loss: 0.616, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch154/200, loss: 0.630, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch155/200, loss: 0.517, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch156/200, loss: 0.291, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch157/200, loss: 0.577, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch158/200, loss: 0.323, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch159/200, loss: 0.100, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch160/200, loss: 0.640, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch161/200, loss: 0.520, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch162/200, loss: 0.674, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch163/200, loss: 0.295, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch164/200, loss: 0.096, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch165/200, loss: 0.295, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch166/200, loss: 0.225, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch167/200, loss: 0.247, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch168/200, loss: 0.612, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch169/200, loss: 0.527, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch170/200, loss: 0.066, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch171/200, loss: 0.372, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch172/200, loss: 1.134, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch173/200, loss: 0.202, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch174/200, loss: 0.539, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch175/200, loss: 0.597, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch176/200, loss: 0.138, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch177/200, loss: 0.182, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch178/200, loss: 0.347, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch179/200, loss: 0.613, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch180/200, loss: 0.998, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch181/200, loss: 0.647, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch182/200, loss: 0.465, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch183/200, loss: 0.651, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch184/200, loss: 0.327, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch185/200, loss: 1.053, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch186/200, loss: 1.315, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch187/200, loss: 0.107, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch188/200, loss: 0.160, Accuracy: 1.000\n",
            "torch.float32\n",
            "Epoch189/200, loss: 0.595, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch190/200, loss: 0.794, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch191/200, loss: 1.021, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch192/200, loss: 0.471, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch193/200, loss: 0.626, Accuracy: 0.750\n",
            "torch.float32\n",
            "Epoch194/200, loss: 0.607, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch195/200, loss: 1.185, Accuracy: 0.000\n",
            "torch.float32\n",
            "Epoch196/200, loss: 0.930, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch197/200, loss: 0.908, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch198/200, loss: 0.648, Accuracy: 0.500\n",
            "torch.float32\n",
            "Epoch199/200, loss: 1.158, Accuracy: 0.250\n",
            "torch.float32\n",
            "Epoch200/200, loss: 0.196, Accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyIiyI11d-vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}